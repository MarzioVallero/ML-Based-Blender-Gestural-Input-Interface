{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gesture detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to be in the root directory of the project before running the following code cells.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the required libraries\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "import numpy as np\n",
    "\n",
    "# Utility definitions\n",
    "WORKSPACE_PATH = 'Tensorflow/Workspace'\n",
    "ANNOTATION_PATH = WORKSPACE_PATH+'/annotations'\n",
    "MODEL_PATH = WORKSPACE_PATH+'/trained-models'\n",
    "PRETRAINED_MODEL_PATH = WORKSPACE_PATH+'/pre-trained-model'\n",
    "CONFIG_PATH = MODEL_PATH+'/custom_trained_model/pipeline.config'\n",
    "CHECKPOINT_PATH = MODEL_PATH+'/custom_trained_model/'\n",
    "actionMap = {\n",
    "    \"SelectPrevious\" : (\"LeftL\", \"RIndexUp\"),\n",
    "    \"SelectNext\" : (\"LeftL\", \"RIndexDown\"),\n",
    "    \"SelectChild\" : (\"LeftL\", \"RIndexRight\"),\n",
    "    \"SelectParent\" : (\"LeftL\", \"RIndexLeft\"),\n",
    "\n",
    "    \"MoveZPositive\" : (\"LeftA\", \"RIndexUp\"),\n",
    "    \"MoveZNegative\" : (\"LeftA\", \"RIndexDown\"),\n",
    "    \"MoveXPositive\" : (\"LeftA\", \"RIndexRight\"),\n",
    "    \"MoveXNegative\" : (\"LeftA\", \"RIndexLeft\"),\n",
    "    \"MoveYPositive\" : (\"LeftA\", \"RIndexFront\"),\n",
    "    \"MoveYNegative\" : (\"LeftA\", \"RThumbBack\"),\n",
    "    \"MoveClear\" : (\"LeftA\", \"RClearC\"),\n",
    "\n",
    "    \"RotateZPositive\" : (\"LeftO\", \"RIndexUp\"),\n",
    "    \"RotateZNegative\" : (\"LeftO\", \"RIndexDown\"),\n",
    "    \"RotateXPositive\" : (\"LeftO\", \"RIndexRight\"),\n",
    "    \"RotateXNegative\" : (\"LeftO\", \"RIndexLeft\"),\n",
    "    \"RotateYPositive\" : (\"LeftO\", \"RIndexFront\"),\n",
    "    \"RotateYNegative\" : (\"LeftO\", \"RThumbBack\"),\n",
    "    \"RotateClear\" : (\"LeftO\", \"RClearC\"),\n",
    "\n",
    "    \"TransformGlobal\" : (\"LeftV\", \"RIndexUp\"),\n",
    "    \"TransformLocal\" : (\"LeftV\", \"RIndexDown\"),\n",
    "    \"ViewNext\" : (\"LeftV\", \"RIndexRight\"),\n",
    "    \"ViewPrevious\" : (\"LeftV\", \"RIndexLeft\"),\n",
    "\n",
    "    \"HideCurrent\" : (\"LeftOpenHand\", \"RIndexRight\"),\n",
    "    \"UnhideCurrent\" : (\"LeftOpenHand\", \"RIndexLeft\"),\n",
    "    \"UnhideAll\" : (\"LeftOpenHand\", \"RIndexUp\"),\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to change the _checkpoint\\_name_ variable to the name of your model's chosen checkpoint name.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'ckpt-37'\n",
    "\n",
    "# Load the pipeline.config file and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore the specified checkpoint (it must match an existing model)\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(CHECKPOINT_PATH, checkpoint_name)).expect_partial()\n",
    "\n",
    "# Computes the detections from the predictive model\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Gestures in Real-Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user should be located at a distance of 1 to 1.5 meters from the camera, else the model won't be able to recognize gestures with high enough accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "import sys\n",
    "import cv2 \n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import tkinter as tk\n",
    "import ast\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from PyQt5 import QtGui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_close(event, cap):\n",
    "    cap.release()\n",
    "\n",
    "# Keyboard interrupt handler\n",
    "def on_press(event):\n",
    "    if event.key == 'q':\n",
    "        print(\"You pressed \" + event.key + \", the program exited\")\n",
    "        cap.release()\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels from the label map\n",
    "category_index = label_map_util.create_category_index_from_labelmap(ANNOTATION_PATH+'/label_map.pbtxt')\n",
    "\n",
    "# Setup camera capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "root = tk.Tk()\n",
    "screen_width = root.winfo_screenwidth()\n",
    "screen_height = root.winfo_screenheight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The window is setup through the matplotlib library.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration file found\n"
     ]
    }
   ],
   "source": [
    "# Setup output window\n",
    "plt.ion()\n",
    "gui = plt.figure(\"Real-time Gesture Detection\", facecolor='#1e1e1e', edgecolor='#1e1e1e')\n",
    "gui.canvas.mpl_connect(\"close_event\", lambda event: handle_close(event, cap))\n",
    "gui.canvas.mpl_connect('key_press_event', on_press)\n",
    "result = None\n",
    "title_obj = plt.title('Real-time Sign Detection')\n",
    "plt.setp(title_obj, color='#d4d4d4')         #set the color of title to white\n",
    "\n",
    "# Check if a configuration file exists, else load a predefined value set\n",
    "if os.path.isfile('Config\\config.dat'):\n",
    "    print(\"Configuration file found\")\n",
    "    file = open(\"Config\\config.dat\", \"r\")\n",
    "    contents = file.read()\n",
    "    config = ast.literal_eval(contents)\n",
    "    file.close()\n",
    "else:\n",
    "    print (\"Configuration file not found. Run CreateHSVProfile.py to create a local profile\")\n",
    "    config = {'HL': 0, 'SL': 29, 'VL': 24, 'HH': 40, 'SH': 255, 'VH': 255}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main camera loop.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You pressed q, the program exited\n"
     ]
    }
   ],
   "source": [
    "# Camera loop\n",
    "while cap.isOpened(): \n",
    "    ret, frame = cap.read()\n",
    "    image_np = np.array(frame)\n",
    "\n",
    "    HSV_Frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    totalMask = cv2.inRange(HSV_Frame, (config[\"HL\"], config[\"SL\"], config[\"VL\"]), (config[\"HH\"], config[\"SH\"], config[\"VH\"]))\n",
    "    totalMask = totalMask.astype(np.uint8)\n",
    "\n",
    "    # The mask finally undergoes the Opening operator in order to remove pepper noise,\n",
    "    # then gets applied as a bitwise operator to the frame\n",
    "    totalMask = cv2.morphologyEx(totalMask, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(7, 7)))\n",
    "    output = cv2.bitwise_and(frame, frame, mask = totalMask)\n",
    "\n",
    "    # The masked image is then converted to a tensor for object detection\n",
    "    # The detections dictionary is formatted according to the objectdetectionAPI\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(output, 0), dtype=tf.float32)\n",
    "    detections = detect_fn(input_tensor)\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                for key, value in detections.items()}\n",
    "    detections['num_qdetections'] = num_detections\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    label_id_offset = 1\n",
    "\n",
    "    # The bounding boxes of all detected gestures are drawn on top of the original frame\n",
    "    # with their corresponding label.\n",
    "    # max_boxes_to_draw=1 doesn't let two overlapping gestures to be recognized at once\n",
    "    # min_score_thresh=.7 ignores all detections with an accuracy rate lower than 70%\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np,\n",
    "                detections['detection_boxes'],\n",
    "                detections['detection_classes']+label_id_offset,\n",
    "                detections['detection_scores'],\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                max_boxes_to_draw=5,\n",
    "                min_score_thresh=.4,\n",
    "                agnostic_mode=False)\n",
    "\n",
    "    # The output is displayed on an interactive window\n",
    "    image_np = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\n",
    "    if result is None:\n",
    "        plt.axis(\"off\")\n",
    "        result = plt.imshow(image_np)\n",
    "        plt.title(\"Real-time Gesture Detection\")\n",
    "        plt.show() \n",
    "    else:\n",
    "        result.set_data(image_np)\n",
    "        gui.canvas.draw()\n",
    "        gui.canvas.flush_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e59b97e49583ef0e2d88c4a0fbba7359baf17716441caa051fc7afb9641bd8af"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
