{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4be1d42e",
   "metadata": {},
   "source": [
    "# Dataset Generation for Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dadac9",
   "metadata": {},
   "source": [
    "First, we add all the useful dependencies for image collection and processing.  \n",
    "Be sure to be in the root directory of the project when executiong the following code cells.  \n",
    "If any of the following imports fail, be sure to install the missing libraries through oyur package manager. I suggest _pip install <library\\_name>_.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f1df295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import uuid\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import shutil\n",
    "import wget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dea5dd",
   "metadata": {},
   "source": [
    "We then define the path of the directory into which we will save and manipulate our dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f3315bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePath = 'Tensorflow/Workspace/images/capturedimages'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9de123",
   "metadata": {},
   "source": [
    "Now we define the list of gesture classes _labels_, which will be used to generate data for the model to train onto.  \n",
    "The _nImages_ variable specifies how many images of a single gesture shall be taken: the higher, the better variance you'll get in the final dataset. However, be warned that all the images for the dataset must be labeled manually, so choose wisely. We will exploit data augmentation to generate a bigger dataset with more variance, in order to improve the final model's generalization capabilities.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5c6191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['LeftL', 'LeftA', 'LeftO', 'LeftV', 'LeftOpenHand', 'RIndexUp', 'RIndexDown', 'RIndexLeft', 'RIndexRight', 'RIndexFront', 'RThumbBack', 'RClearC']\n",
    "nImages = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426de088",
   "metadata": {},
   "source": [
    "We define some data aumentation functions to produce multiple output images from a single input.  \n",
    "Since the gestures we want to recognize are hand-bound, since there are gestures exclusive to the right or left hand, we cannot perform horizontal mirroring. Vertical mirroring and rotations dont't make any sense in this case, since all the gestures are direction bound, so such transformations could make two different classes overlap with each other.  \n",
    "The only useful data augmentation procedures we can exploit are thus vertical and horizontal shifts (or stretchs) and zoom transformations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b68a621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertical_shift(input_img, ratio=0.2):\n",
    "    if ratio > 1 or ratio < 0:\n",
    "        print('Value should be less than 1 and greater than 0')\n",
    "        return input_img\n",
    "    img = input_img.copy()\n",
    "    ratio = random.uniform(-ratio, ratio)\n",
    "    h, w = img.shape[:2]\n",
    "    to_shift = h*ratio\n",
    "    if ratio > 0:\n",
    "        img = img[:int(h-to_shift), :, :]\n",
    "    if ratio < 0:\n",
    "        img = img[int(-1*to_shift):, :, :]\n",
    "    img = cv2.resize(img, (w, h), cv2.INTER_CUBIC)\n",
    "    return img\n",
    "\n",
    "def horizontal_shift(input_img, ratio=0.2):\n",
    "    if ratio > 1 or ratio < 0:\n",
    "        print('Value should be less than 1 and greater than 0')\n",
    "        return input_img\n",
    "    img = input_img.copy()\n",
    "    ratio = random.uniform(-ratio, ratio)\n",
    "    h, w = img.shape[:2]\n",
    "    to_shift = w*ratio\n",
    "    if ratio > 0:\n",
    "        img = img[:, :int(w-to_shift), :]\n",
    "    if ratio < 0:\n",
    "        img = img[:, int(-1*to_shift):, :]\n",
    "    img = cv2.resize(img, (w, h), cv2.INTER_CUBIC)\n",
    "    return img\n",
    "\n",
    "def zoom(input_img, value=0.85):\n",
    "    if value > 1 or value < 0:\n",
    "        print('Value for zoom should be less than 1 and greater than 0')\n",
    "        return input_img\n",
    "    img = input_img.copy()\n",
    "    value = random.uniform(value, 1)\n",
    "    h, w = img.shape[:2]\n",
    "    h_taken = int(value*h)\n",
    "    w_taken = int(value*w)\n",
    "    h_start = random.randint(0, h-h_taken)\n",
    "    w_start = random.randint(0, w-w_taken)\n",
    "    img = img[h_start:h_start+h_taken, w_start:w_start+w_taken, :]\n",
    "    img = cv2.resize(img, (w, h), cv2.INTER_CUBIC)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d9a897",
   "metadata": {},
   "source": [
    "The following code box will warn you about which gesture must be performed. The delay between each frame capture is of 2 seconds, but can be freely adjusted to whatever you need.  \n",
    "Before the start of the capturing procedure for each class, the user is prompted to press Enter in an input box, to improve usability and provide mid-capture pause scenarios.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba14767d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting images for LeftA\n"
     ]
    }
   ],
   "source": [
    "frame_capture_delay = 2 #seconds\n",
    "for label in labels:\n",
    "    !mkdir {'Tensorflow\\Workspace\\images\\capturedimages\\\\' + label}\n",
    "    clear_output(wait=True)\n",
    "    print('Collecting images for {}'.format(label))\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    input()\n",
    "    for imageNumber in range(nImages):\n",
    "        ret, frame = cap.read()\n",
    "        tmpUUID = format(str(uuid.uuid1()))\n",
    "        imageName = os.path.join(imagePath, label, label+'_'+f'{tmpUUID}.jpg')\n",
    "        cv2.imwrite(imageName, frame)\n",
    "        print('Collected')\n",
    "        cv2.imshow('frame', frame)\n",
    "        cv2.imwrite(os.path.join(imagePath, label, label+'_vshift_'+f'{tmpUUID}.jpg'),\n",
    "                    vertical_shift(input_img=frame))\n",
    "        cv2.imwrite(os.path.join(imagePath, label, label+'_hshift_'+f'{tmpUUID}.jpg'),\n",
    "                    horizontal_shift(input_img=frame))\n",
    "        cv2.imwrite(os.path.join(imagePath, label, label+'_zoom_'+f'{tmpUUID}.jpg'),\n",
    "                    zoom(input_img=frame))\n",
    "        time.sleep(frame_capture_delay)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            cv2.destroyWindow('frame')\n",
    "            break\n",
    "    cap.release()\n",
    "cv2.destroyWindow('frame')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f3d108",
   "metadata": {},
   "source": [
    "At the end of the capture step, go into the _capturedimages_ directory.  \n",
    "Check all the sub-directories generated output images, to ensure that all of them contain well structured data: if not, repeat the capture process by specifying only the number of images and classes you need to replace.  \n",
    "\n",
    "The following cell will then move all the images from the class sub-directories to the _capturedimages_ directory.  \n",
    "If you redefined the _labels_ list to capture some extra images, be sure to redefine the _labels_ list before executing the following cell, so that all files get correctly moved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e827eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels:\n",
    "    source_dir = imagePath + f\"/{label}/\"\n",
    "    file_names = os.listdir(source_dir)\n",
    "    for file_name in file_names:\n",
    "        # Remove comment for verbose output\n",
    "        #print(f\"Moving {file_name} from {source_dir} to {imagePath}\")\n",
    "        shutil.move(os.path.join(source_dir, file_name), imagePath)\n",
    "    os.rmdir(source_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d330a2c7",
   "metadata": {},
   "source": [
    "# Dataset labeling\n",
    "\n",
    "All commands must be run from the root directory of the project, unless otherwise stated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c21b772",
   "metadata": {},
   "source": [
    "## Path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e9d5328",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE_PATH = 'Tensorflow/Workspace'\n",
    "SCRIPTS_PATH = 'Tensorflow/ConversionScript'\n",
    "APIMODEL_PATH = 'Tensorflow/models'\n",
    "ANNOTATION_PATH = WORKSPACE_PATH+'/annotations'\n",
    "IMAGE_PATH = WORKSPACE_PATH+'/images'\n",
    "MODEL_PATH = WORKSPACE_PATH+'/trained-models'\n",
    "PRETRAINED_MODEL_PATH = WORKSPACE_PATH+'/pre-trained-model'\n",
    "CONFIG_PATH = MODEL_PATH+'/custom_trained_model/pipeline.config'\n",
    "CHECKPOINT_PATH = MODEL_PATH+'/custom_trained_model/'\n",
    "CUSTOM_MODEL_NAME = 'custom_trained_model' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab12ba0",
   "metadata": {},
   "source": [
    "The label map hereby generated contains all classes we want to train our model onto and assigns an integer id to each one.  \n",
    "The map can be modified in order to account for number of different labels, just be sure that it features exactly the same classes previously defined in the _labels_ list.  \n",
    "The code box down below will generate the _label_map.pbtxt_ file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea0b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir \"{ANNOTATION_PATH}\"\n",
    "label_map = []\n",
    "id = 1\n",
    "for label in labels:\n",
    "    label_map.append({'name':f\"{label}\", 'id':f\"{id}\"})\n",
    "    id = id + 1\n",
    "\n",
    "with open(ANNOTATION_PATH + '\\label_map.pbtxt', 'w') as f:\n",
    "    for entry in label_map:\n",
    "        f.write('item { \\n')\n",
    "        f.write('\\tname:\\'{}\\'\\n'.format(entry['name']))\n",
    "        f.write('\\tid:{}\\n'.format(entry['id']))\n",
    "        f.write('}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb01e918",
   "metadata": {},
   "source": [
    "## Label images with LabelImg\n",
    "\n",
    "The follwing code cell moves the notebook's shell to the LabelImg directory and runs LabelImg.  \n",
    "\n",
    "In the LabelImg instance that just opened, be sure to check in the upper left menu _View->Auto Save Mode_.  \n",
    "Click onto _Open Dir_ on the left vertical menu bar and select the _collected images_ directory, then _Select directory_.  \n",
    "Click onto _Change Save Dir_ on the left menu bar and select the _collected images_ directory, then _Select directory_.  \n",
    "\n",
    "You should now see all the images you previously captured.  \n",
    "Use _w_ to activate the draw bounding box utility and select the gesture in the image, confirming by clicking with the left mouse button. A small input window will open, asking you to assign a name to the object class for the bounding box you just drew: for consistency's sake, we used the same names as for the _labels_ list.  \n",
    "Repeat the process until all the images have been labeled with exactly one label. It's possible to specify multiple bounding boxes for multiple elements in a single image, however this would make the dataset more complex to manage. Forthis reason, in this case, each dataset entry will contain one single gesture.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63beb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"labelImg\"\n",
    "!pyrcc5 -o libs/resources.py resources.qrc # Solves \"No module named 'libs.resources'\" bug, if present\n",
    "!python labelImg.py\n",
    "%cd \"..\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecd828c",
   "metadata": {},
   "source": [
    "## Generate Tensorflow .record files\n",
    "\n",
    "First, we download the _generate\\_tfrecord.py_ script from the Tensorflow Object Detection API page.  \n",
    "The script will index the Train and Test partitions of the dataset, such that the training script can find the files.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c912cc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tensorflow/ConversionScript/generate_tfrecord.py'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/_downloads/da4babe668a8afb093cc7776d7e630f3/generate_tfrecord.py'\n",
    "wget.download(url, out=SCRIPTS_PATH, bar=wget.bar_thermometer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e72e38e",
   "metadata": {},
   "source": [
    "We then must split our dataset into a training and a test partition. Be sure to evenly distribute samples for each class among the two sets, or else the training may yield underfitting or overfitting problems.  \n",
    "We will use 25% of the dataset for the test partition and the remaining 75% for the testing partition. Since we have 100 samples for each class, it amounts to 25 test samples for each class.  \n",
    "We won't use a validation partition, since the original model's structure from which we are performing transfer learning doesn't support it.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86d4e61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_test_samples = 25\n",
    "!mkdir \"{IMAGE_PATH + '/test'}\"\n",
    "!mkdir \"{IMAGE_PATH + '/train'}\"\n",
    "for label in labels:\n",
    "    file_names = [file for file in os.listdir(imagePath)\n",
    "                  if file.endswith(\"jpg\") if file.startswith(label + \"_\")]\n",
    "    random.shuffle(file_names)\n",
    "    index = 0\n",
    "    for file_name in file_names:\n",
    "        pre, ext = os.path.splitext(file_name)\n",
    "        annotation_name = pre + \".xml\"\n",
    "        if(index < number_test_samples):\n",
    "            # Remove comment for verbose output\n",
    "            #print(f\"Moving {file_name} and {annotation_name} from {imagePath} to {IMAGE_PATH + '/test'}\")\n",
    "            shutil.copy(src=imagePath + \"/\" + file_name, dst=IMAGE_PATH + '/test')\n",
    "            shutil.copy(src=imagePath + \"/\" + annotation_name, dst=IMAGE_PATH + '/test')\n",
    "            #shutil.move(os.path.join(source_dir, file_name), imagePath)\n",
    "        else:\n",
    "            # Remove comment for verbose output\n",
    "            #print(f\"Moving {file_name} and {annotation_name} from {imagePath} to {IMAGE_PATH + '/train'}\")\n",
    "            shutil.copy(src=imagePath + \"/\" + file_name, dst=IMAGE_PATH + '/train')\n",
    "            shutil.copy(src=imagePath + \"/\" + annotation_name, dst=IMAGE_PATH + '/train')\n",
    "        index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc25b367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the TFRecord file: Tensorflow/Workspace/annotations/train.record\n",
      "Successfully created the TFRecord file: Tensorflow/Workspace/annotations/test.record\n"
     ]
    }
   ],
   "source": [
    "!python {SCRIPTS_PATH + '/generate_tfrecord.py'} -x {IMAGE_PATH + '/train'} -l {ANNOTATION_PATH + '/label_map.pbtxt'} -o {ANNOTATION_PATH + '/train.record'}\n",
    "!python {SCRIPTS_PATH + '/generate_tfrecord.py'} -x{IMAGE_PATH + '/test'} -l {ANNOTATION_PATH + '/label_map.pbtxt'} -o {ANNOTATION_PATH + '/test.record'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d678143d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
